# 支持向量机

## 1、线性可分支持向量机与硬间隔最大化。

考虑一个二分类问题。输入空间与特征空间为两个不同的空间。线性可分支持向量机、线性支持向量机假设两个空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量。非线性支持向量机利用一个非线性映射，将输入映射为特征向量。

**支持向量机的学习是在特征空间中进行的。**

**特点：假设数据集线性可分，支持向量机采用间隔最大化来求最优分离超平面，此时解是唯一的。**



**定义1（线性可分支持向量机）**

给的线性可分训练数据集，通过间隔最大化或等价地求解相应的凸二次规划问题学习得到的分离超平面为
$$
w^* \cdot x + b^* =0
$$
以及相应的分类决策函数
$$
f(x)=sign(w^* \cdot x + b^*)
$$
称为线性可分支持向量机。



**定义2（函数间隔）**
$$
到某个点:\hat \gamma_i = y_i (\omega \cdot x_i +b) 
\\
到某个训练集:\hat \gamma = min \hat \gamma_i
$$
**定义3（几何间隔）**
$$
\gamma_i = \frac{\hat \gamma_i}{||\omega||}  \\
\gamma = \frac{\hat \gamma}{||\omega||} \\
其中||\omega|| = |\omega \cdot x + b|
$$
**支持向量机学习的基本想法是求解能够正确划分训练集并且几何间隔最大的分离超平面**





即求
$$
\mathop{\min}_{w, b} {\gamma}  \\
s.t. \quad  y_i(\frac{\omega}{||\omega||}\cdot x_i + \frac{b}{||\omega||}) \geq \gamma \\
$$
等价于
$$
\mathop{\min}_{w, b} {\frac {\hat \gamma}{||\omega||}}  \\
s.t. \quad  y_i(\omega \cdot x_i + b) \geq \hat \gamma \\
$$
由于函数间隔的取值大小并不影响超平面的选取，即可取函数间隔为1，得到
$$
\mathop{\min}_{w, b} {\frac {1}{||\omega||}} <=> \mathop{\min}_{w,b} \frac{1}{2}||\omega||^2  \\
s.t. \quad  y_i(\omega \cdot x_i + b) - 1\geq 0 \\
$$
即一个凸二次规划问题。 求解即可得到分隔超平面。



由此得到

**算法1（线性可分支持向量机学习算法 ----- 最大间隔法）**

